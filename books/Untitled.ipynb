{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3092c650-e0ad-49f7-9eb8-83a303988c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import streamlit as st\n",
    "import fitz\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import ConversationalRetrievalChain, LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.schema import Document\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain.chains.combine_documents.map_reduce import MapReduceDocumentsChain\n",
    "from IPython.display import Markdown, display, update_display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b0efad6-cd0e-4028-836a-fca8e5ecadf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override = True)\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai = OpenAI()\n",
    "llm=ChatOpenAI(model = 'gpt-4o-mini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "528e2c34-b738-4c67-997c-751fc66d73d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_with_metadata(pdf_folders):\n",
    "    documents = []\n",
    "    for filename in os.listdir(pdf_folders):\n",
    "        if filename.lower().endswith(\".pdf\"):\n",
    "            pdf_path = os.path.join(pdf_folders, filename)\n",
    "            doc = fitz.open(pdf_path)\n",
    "\n",
    "            full_text = \"\"\n",
    "            for page in doc:\n",
    "                full_text += page.get_text()\n",
    "\n",
    "            if full_text.strip():\n",
    "                documents.append(Document(\n",
    "                    page_content=full_text,\n",
    "                    metadata={\"source\": filename}\n",
    "                ))\n",
    "                print(f\"Loaded: {filename}\")\n",
    "            else:\n",
    "                print(f\"Skipped (no text): {filename}\")\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e576942-a29c-4307-9fd8-435e6017125c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitter\n",
    "def splitter_chunk_with_metadata(pages):\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=50\n",
    "    )\n",
    "\n",
    "    all_chunks = []\n",
    "    for page in pages:\n",
    "        docs = splitter.create_documents([page.page_content])\n",
    "        for i , chunk in enumerate(docs):\n",
    "            chunk.metadata = page.metadata.copy()\n",
    "            chunk.metadata[\"chunk_id\"]=i\n",
    "            all_chunks.append(chunk)\n",
    "    return all_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7dc04f1-308a-48c2-bfe5-1bad8d71d058",
   "metadata": {},
   "outputs": [],
   "source": [
    "strict_prompt = PromptTemplate(\n",
    "    input_variables =[\"context\",\"question\"],\n",
    "    template=\"\"\"\n",
    "    You are a PDF assistant. Use ONLY the context below to answer the question.\n",
    "    If the answer is not in the context, respond with: \"The answer is not provided in the PDFs\"\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question:\n",
    "    {question}\n",
    "\n",
    "    Answer:\n",
    "    \"\"\"\n",
    ")\n",
    "loose_prompt_template = \"\"\"\n",
    "    You are a helpful assistant. Use the context below to answer the question as best as you can.\n",
    "    You can infer and reason using your own understanding, but do not make things up.\n",
    "    \n",
    "    Context:\n",
    "    {context}\n",
    "    \n",
    "    Question: {question}\n",
    "    Answer:\"\"\"\n",
    "\n",
    "loose_prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=loose_prompt_template\n",
    ")\n",
    "\n",
    "def get_strict_chain(llm,retriever):\n",
    "    return RetrievalQA.from_chain_type(\n",
    "        llm=ChatOpenAI(model = 'gpt-4o-mini'),\n",
    "        retriever=retriever,\n",
    "        chain_type=\"stuff\",\n",
    "        chain_type_kwargs={\"prompt\":strict_prompt},\n",
    "        return_source_documents=True\n",
    "    )\n",
    "def get_loose_chain(llm, retriever):\n",
    "    return RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        retriever=retriever,\n",
    "        chain_type=\"stuff\",  \n",
    "        return_source_documents=False,  \n",
    "        chain_type_kwargs={\"prompt\":loose_prompt}\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "583cbb10-208f-4f8b-8399-d25c82704971",
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_answer_with_llm(question, initial_answer, llm):\n",
    "    review_prompt = f\"\"\"\n",
    "You are a helpful AI assistant.\n",
    "\n",
    "Here is a question and its original answer based only on some context.\n",
    "\n",
    "Question: {question}\n",
    "Answer: {initial_answer}\n",
    "\n",
    "Does this answer directly address the question? If not, explain briefly and then rewrite the answer to be more helpful and complete, using your own understanding.\n",
    "If it’s already good, just say: \"The answer is good.\".\n",
    "\"\"\"\n",
    "    return llm.invoke(review_prompt).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "91b06ca3-384c-414b-bc69-68ff2b2f1110",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(mode, query):\n",
    "    if mode == \"strict\":\n",
    "        result = strict_chain({\"query\": query})\n",
    "        print(\"\\n[STRICT MODE]\")\n",
    "        print(\"Answer:\", result[\"result\"])\n",
    "        print(\"Sources:\")\n",
    "        for doc in result[\"source_documents\"]:\n",
    "            print(f\" - {doc.metadata.get('source', 'Unknown')}\")\n",
    "\n",
    "    elif mode == \"loose\":\n",
    "        result = loose_chain.run(query)\n",
    "        print(\"\\n[LOOSE MODE]\")\n",
    "        print(\"Answer:\", result)\n",
    "\n",
    "    elif mode == \"enhanced\":\n",
    "        response = strict_chain({\"query\":query})\n",
    "        result = response[\"result\"]\n",
    "        llm=ChatOpenAI(model = 'gpt-4o-mini')\n",
    "        reviewed = review_answer_with_llm(\n",
    "            question=query,\n",
    "            initial_answer=result,\n",
    "            llm=llm)\n",
    "        print(\"\\n[ENHANCED MODE]\")\n",
    "        print(\"\\nLLM Review Output:\\n\", reviewed)\n",
    "        print(\"\\nSources:\")\n",
    "        for doc in response[\"source_documents\"]:\n",
    "            print(f\" - {doc.metadata.get('source', 'Unknown')}\")\n",
    "        \n",
    "\n",
    "    else:\n",
    "        print(\"Invalid mode. Use 'strict' or 'loose'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2a295f29-6e09-4860-968f-f18d3aefa6d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a33f0c68-3673-4aed-8733-55506db3f3d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: Abnormal CXRs_compressed (1).pdf\n",
      "Loaded: Abnormal CXRs_compressed.pdf\n",
      "Loaded: Introduction to interpretation of chest radiograph – Normal radiological anatomy_compressed.pdf\n",
      "Loaded: Lecture Note on Spine.pdf\n",
      "Loaded: RAD 404 - Technique III.pdf\n",
      "Loaded: RAD 409-The chemistry of developer solution.pdf\n",
      "Loaded: RAD 416 -Treatment Machines_compressed.pdf\n",
      "Loaded: RAD 420-Plain Abdomen_compressed.pdf\n",
      "Loaded: RAD 420-The Spine_compressed.pdf\n",
      "Skipped (no text): RAD ANATOMY A D PATHOLOGY BY KAY C.pdf\n",
      "Loaded: The Research Process_compressed-1.pdf\n",
      "Loaded: Writing a Research Proposal_compressed.pdf\n"
     ]
    }
   ],
   "source": [
    "pdf_folders = r\"C:\\Users\\User\\Documents\\Projects\\llm_engineering\\Mini-search-engine\\books\"\n",
    "pages = extract_text_with_metadata(pdf_folders)\n",
    "chunks = splitter_chunk_with_metadata(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "be67f32e-cda7-4291-b277-1786bb8ec8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = OpenAIEmbeddings()\n",
    "\n",
    "vector_db = Chroma.from_documents(chunks, embedding)\n",
    "retriever = vector_db.as_retriever(search_type=\"mmr\", search_kwargs={\"k\":6})\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "strict_chain = get_strict_chain(llm, retriever)\n",
    "loose_chain = get_loose_chain(llm, retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "72101325-52ae-46b9-b255-ac4d277c4c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STRICT MODE]\n",
      "Answer: The answer is not provided in the PDFs.\n",
      "Sources:\n",
      " - RAD 409-The chemistry of developer solution.pdf\n",
      " - RAD 416 -Treatment Machines_compressed.pdf\n",
      " - RAD 420-The Spine_compressed.pdf\n",
      " - RAD 420-Plain Abdomen_compressed.pdf\n",
      " - RAD 416 -Treatment Machines_compressed.pdf\n",
      " - RAD 416 -Treatment Machines_compressed.pdf\n"
     ]
    }
   ],
   "source": [
    "ask_question(\"strict\",\"who is the father of radiography?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d69954-a252-4e1a-ba4d-0fce7ee865c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
